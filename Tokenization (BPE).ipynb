{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcf0e403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for special tokens\n",
    "class Handle_special():\n",
    "    def __init__(self, allowed_special, ids, vocab_length):\n",
    "        self.allowed_special = allowed_special\n",
    "        self.ids = ids\n",
    "        self.vocab_length = vocab_length\n",
    "        self.special_dict = {}\n",
    "        \n",
    "    #replace sequence\n",
    "    def replace_sequence(self, id_lst, target, replacement):\n",
    "        n = len(target)\n",
    "        i=0\n",
    "        if isinstance(replacement, list):\n",
    "            m = len(replacement)\n",
    "        else:\n",
    "            m=1\n",
    "            replacement = [replacement]\n",
    "        while i <= len(id_lst)-n+1:\n",
    "            if id_lst[i:i+n]==target:\n",
    "                id_lst[i:i+n]=replacement\n",
    "                i+=m\n",
    "                #print(\"replcaing\")\n",
    "            else:\n",
    "                i+=1\n",
    "\n",
    "        return id_lst\n",
    "    #small vocab for special tokens\n",
    "    def special_tokens(self):\n",
    "        special_ids = [tuple(map(int, i.encode('utf-8'))) for i in self.allowed_special]\n",
    "        j=0\n",
    "        for id_ in special_ids:\n",
    "            self.special_dict[id_] = self.vocab_length+j\n",
    "            j+=1\n",
    "        #return special_dict\n",
    "\n",
    "    #small\n",
    "    def add_tokens(self):\n",
    "        newer = self.ids.copy()\n",
    "        #print(newer)\n",
    "        #print(len(newer))\n",
    "        #print(\"-\"*50)\n",
    "        self.special_tokens() #populate special_dict\n",
    "        for k in self.special_dict.keys():\n",
    "            newer = self.replace_sequence(newer, list(k), self.special_dict[k])\n",
    "        return newer\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9e5cea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#usage\n",
    "#encoder = MyTokenizer(corpus, vocab_length)\n",
    "#encoder.train()\n",
    "#then u can use it to enccode and decode \n",
    "\n",
    "class MyTokenizer():\n",
    "    def __init__(self, corpus, vocab_size=1000, allowed_special=None):\n",
    "        self.corpus = corpus\n",
    "        self.vocab_size = vocab_size\n",
    "        self.merges = {} # (int, int) -> int\n",
    "        self._vocab = {idx:bytes([idx]) for idx in range(256)}\n",
    "        self.allowed_special = allowed_special #list of special tokens\n",
    "        self.h=None\n",
    "        \n",
    "    def _get_stats(self, ids):\n",
    "        counts = {}\n",
    "        for pair in zip(ids, ids[1:]): #pythonic way to iterate over \n",
    "            counts[pair] = counts.get(pair, 0) + 1\n",
    "        return counts\n",
    "    \n",
    "    def _merge(self, ids, pair, idx):\n",
    "        newids = []\n",
    "        i=0\n",
    "        while i<len(ids):\n",
    "            #if not at last position and pair matches, replace it\n",
    "            if i<len(ids)-1 and ids[i]==pair[0] and ids[i+1] ==pair[1]:\n",
    "                newids.append(idx)\n",
    "                i+=2\n",
    "            else:\n",
    "                newids.append(ids[i])\n",
    "                i+=1\n",
    "        return newids\n",
    "    \n",
    "    def train(self):\n",
    "        tokens = self.corpus.encode(\"utf-8\") #raw bytes\n",
    "        tokens = list(map(int, tokens))\n",
    "        \n",
    "        ids = list(tokens) # copy not to destroy orig list\n",
    "        self.h = Handle_special(self.allowed_special, ids,self.vocab_size)\n",
    "        ids = self.h.add_tokens() # merge special tokens and return new tokens\n",
    "        #\n",
    "        # add new tokens to end of vocab\n",
    "        spec_dict =  self.h.special_dict\n",
    "        for i in list(spec_dict.keys()):\n",
    "            v = b''\n",
    "            for j in range(len(i)):\n",
    "                v+=self._vocab[i[j]]\n",
    "            self._vocab[spec_dict[i]]=v\n",
    "\n",
    "        \n",
    "        num_merges = self.vocab_size - 256\n",
    "\n",
    "\n",
    "        #for each run, get top pair and replace it\n",
    "        for i in range(num_merges):\n",
    "            stats = self._get_stats(ids)\n",
    "            # Check if stats is empty\n",
    "            if not stats:\n",
    "                print(f\"No more pairs to merge at iteration {i}.\")\n",
    "                print(f\"You are overfitting, reduce the vocabulary size to an appropriate one\")\n",
    "                break\n",
    "            pair = max(stats, key = stats.get)\n",
    "            idx = 256+i\n",
    "            #print(f\"merging {pair} into new token {idx}\")\n",
    "            ids = self._merge(ids, pair, idx) #replcae occuances\n",
    "            self.merges[pair] = idx\n",
    "            if i%100==0:\n",
    "                print(f\"Merging the {i}th pair\")\n",
    "            \n",
    "        \n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            self._vocab[idx] = self._vocab[p0]+self._vocab[p1]\n",
    "\n",
    "        \n",
    "            \n",
    "        print(\"tokens length: \", len(tokens))\n",
    "        print(\"ids length: \", len(ids))\n",
    "        print(f\"compression ratio: {len(tokens)/len(ids):.2f}X\")\n",
    "\n",
    "        \n",
    "    def decode(self,ids): #enter list of tokens i.e [23,44,55]\n",
    "        tokens = b\"\".join(self._vocab[idx] for idx in ids)\n",
    "        text = tokens.decode(\"utf-8\", errors = \"replace\")\n",
    "        return text\n",
    "    \n",
    "    def encode(self, text): #enter text\n",
    "        tokens = list(text.encode('utf-8'))\n",
    "        #first replace the special tokens if present\n",
    "        new_tokens = list(tokens)\n",
    "        for i in range(len(self.allowed_special)):\n",
    "            target = tuple(map(int,self.allowed_special[i].encode('utf-8')))\n",
    "            new_tokens = self.h.replace_sequence(new_tokens, list(target), self.h.special_dict[target])\n",
    "        \n",
    "        print(\"Length of original tokens: \",len(tokens))\n",
    "        print(\"Length of tokens after handling special words: \", len(new_tokens))\n",
    "        \n",
    "        tokens = list(new_tokens)\n",
    "        while len(tokens)>=2:\n",
    "            stats = self._get_stats(tokens)\n",
    "\n",
    "            #we gonna check pair with minimum value in vocab, if it exist the first one actually\n",
    "            #until all are done\n",
    "            pair = min(stats, key=lambda p:self.merges.get(p, float(\"inf\")))\n",
    "\n",
    "            if pair not in self.merges:\n",
    "                break #nothing else to merge, break\n",
    "            idx = self.merges[pair]\n",
    "            tokens = self._merge(tokens, pair, idx)\n",
    "        print(\"Length of tokens after merging: \",len(tokens))\n",
    "        return tokens\n",
    "    \n",
    "    def get_vocabulary(self):\n",
    "        return dict(sorted(self._vocab.items()))\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "472ed425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a longer text\n",
    "text = \"\"\"Mikwano gyange bwoya gitera okumbuuza engeri amaaso gange agaali ag'empujjo gye \n",
    "gaateereeramu are ne bwe nakutuka ekiwalaata eky'embagirawo ekitaali mu lulyo ate \n",
    "n'ennyindo n'ensongola. Okubannyonnyola bino nga bwe byajja nali nteekwa okubawayiza \n",
    "ebyantuukako mu sematalo on owa jjo lya balamu bye nali nnyimye amazzi. Naye no bwe \n",
    "nabinyumizaako kagafumba ne nfabulago, bo kwe kuntayirira mbinyumizeeko n'abalala era \n",
    "baludde ddaaki ne bankuulamu omwasi.|<eos>| \n",
    "Be twasomanga nabo bammanyi nga Kakundugulu, erinnya lye bampaatiikako ne lisimba \n",
    "emmizi, ate bo bwe twazirwanako bammanyi nga Kasiribiti. Eby'e Kololo nga Sargent  ŋŋoŋo \n",
    "simusudde, Tororo, Giligiri, Mbagasi ne Gonda ebyo byo mbirekedde bannange abalala \n",
    "babirombojjeko naye ebyange nja kubitandikira mu nsi z’Abawalabu.|<eos>| \n",
    "Nga tumaze okukekeza ennyago mu nsi nnyingi, twatuuka mu nsi z’Abawarabu gye \n",
    "nnyingirira mu kitongole ekikessi. Bwe twamala okutendekebwa, okubangulwa era \n",
    "nokukenkusibwa mu katoola w’ebintu ne tubuuzibwa ebigezo ebyalumya buli omu \n",
    "ogwengulu. Bwe kityo nga abamu tumaze okubikuba oluku mu mutwe n'abalala nga \n",
    "bibatudde ku nfeete, Sargent Kwi Tamutaamu omuyugoyugo era eyali omugabe waffe Kopolo \n",
    "Monyo Odeku, Figo Mukombankuyege Kataayi, Palalemo Byomera, Ludyeku era nange \n",
    "kamwakoogera, twalondebwa mu gubinja gwa bantu nga kinaana tugende tukette mu bizinga \n",
    "bye Yugoyugo ebyetondese mu bwengula bweriyanja li Pasifika (Pacific Ocean) ebyali \n",
    "biwambiddwa aba Japan era kwe basinziiranga okutagenya amaato ga b'amawanga \n",
    "amagatte. Okuggyako nze, bannange bonna baali njasabiggu za basajja abatayisikamu maaso \n",
    "wadde okuwetwamu ennoga.|<eos>| \n",
    "Buli omu ku ffe yalina kyatwalirirwa okuketta. Nze nnabinikibwako kuketta gusaawe \n",
    "gw’ennyonyi ogwaliyo ngukube ebifaananyi era nekenneenye era nkube ezigubuukako \n",
    "ebifaananyi  ate era nekkaanye embeera n’enkyukakyuka z’obudde mu bitundu ebyo. \n",
    "Naweebwa emyezi ebiri gyokka okutuukiririzaamu bino byonna. Nali Oluyugoyugo ndukuba \n",
    "budinda era nga n’Olujapaani nkusuwazizaamu.|<eos>| \n",
    "Mu matulutulu g’olunaku olumu, awatali na kusooka kubbirwako, twawawamulwa mu tulo \n",
    "okunyanyagira kw’eŋŋombe era okugenda okwesimba mu luggya lw’ensiisira zaffe nga tulaba \n",
    "gamotoka galwanyi ge gakungubadde. Twayambazibwa kipaku mu byambalo by’ekiwarabu. \n",
    "Amakanzu gwatwambikiddwa, ebimyu bimyuumyuddwa, eminagiro gisuuliddwa, agalemba \n",
    "galinnye ku mitwe, balikkupu mususaane zinaanikiddwa, amasulubu n’amalevu \n",
    "amajingejinge gatujenjekeddwaako era n’amannya g’ekiwarabu gatupaatikiddwaako. Nze \n",
    "munnowo nga mpitibwa Abdulla Hussein. Mu ndabika yonna nga otulengedde wamma ggwe \n",
    "nga ffe Bawarabu ba nnakabala ggeregere awatali kuwunnaanya.\n",
    "Okwesiiga gerenge kwatutwalira akaseera akatabandaaza na mbooge. Bwe kutyo nga \n",
    "kuwedde, twesolossa mu gamotoka agalwanyi. Olwali okuggweeramu tuti nga tweggyawo. \n",
    "Twagawuliranga nga gagenda gawenyuka, gagenda gawuluguma, gatokota, gaguluba, \n",
    "gayuuga era gebbinkiza, gasituka naye okukugamba nti twalengerako gyetuva oba gyetulaga, \n",
    "oba wadde okulaba akamunyeenya obumunyeenyi, olwo ekkubo eryange liba ku lusebenju \n",
    "nga lye bagamba ery’omulimba. Munda mwe twali nga mukutte be kwatekwate, mufunda, \n",
    "twezinze bugongolo era nga n’entuuyo zituttulukuka nga abatudde ku kabiga.\n",
    "Mu ttuntu wakati twagguka ku kaalo keetikkiddwa enfuufu olwa kikunta eyali yekaaliisa era \n",
    "ke nateebereza okubeera ku nnyanja emmyuufu. Abantu abaakalimu baali ba munaganwa \n",
    "ngoyinza okubabalira ku ngalo. Saabetegera nakatono kubanga olwali okwesowola mu \n",
    "gumotoka nga bampalampya bukwakku mu kaato akaali ake kika ekyezi ky’emitego era ne \n",
    "bannange bwebatyo bwe baabakola. Ffenna okugenda okuggweerako nga nga kaatandise dda \n",
    "kusiikuula nkasi .|<eos>| \n",
    "Bwe twatuula mu nnyanja ye Buyindi (Indian Ocean) ne tumaamulwako ebyambalo \n",
    "by’eKiwarabu byonna era manalevu gaffe ge twali tutandise okunyumirwa ne galugenda. \n",
    "Twayambazibwa ebyambalo byekirunnyanja ne tuzzibwa ne mulyato eddala ery’ekika \n",
    "ekirawunyi. Mu kutyekula emisinde nga lino eryato lye lisinga amalala gonna agaali awo. \n",
    "Omuyaga oba engezi bwe byabanga tebiriboyaanya oba okulisunda oba okulizinyisa oluguje, \n",
    "nga tukkirizibwa okudaaladaalako waggulu ku lyo naye naye nga tetukkirizibwa kulasa \n",
    "mboozi wadde okuwaya n’omulunnyanja yenna. Kye siyinza kwerabira gwe muyaga oguli ku \n",
    "nnyanja eno anti buli kiseera nga gwesooza bwesooza.|<eos>| \n",
    "Nga tumaze ennaku nga tupekukira ku nnyanja eno, twayingirira eriyanja li kiri mulaala li \n",
    "Pasifika ate netuzzibwa ne mu gwato ogubuukirwako ennyonyi ogw’Abamerika. Olwo nno \n",
    "nga twenwanwagirizza ne mu byambalo by’Amerika. Ebyekijaasi bye bye baatunaanika bwe \n",
    "baatujjaako ebyekirunnyanja. Lumu ennyonyi za Japan nga ziwagirwa obwato bu lubbira \n",
    "zaalumba ne zitigomya ogwato ogwo okuva enkya okusuulira ddala enjuba, kyokka \n",
    "olwokutuntumula agazinga gaagwo obutasalako n’olwokuteregenya ennyo nga \n",
    "guwunjawunja ate nga nennyozi zaagwo engabo zigirumizza mannyo era nga zittunsa \n",
    "agakoomi gomukka okugusiikiriza, gwasobola okwerwanako okutuusa ekiyamba mumizi lwe \n",
    "kyagudduukirira ne guzeemululako ne gubula. Naye nga ddala kwali kuyita mu mannyo ga \n",
    "ntaggya. Ekiseera kyonna enkalu we zakalambirira nga ffe buli omu afuuyirira kanwe nti sso \n",
    "anaabuzzaako ddi eddiba ne twesoobolola ku lukokobe era ddala ku lukokere lwomulabe. \n",
    "Buli omu nno nga yezingiridde ekikoba ekiseeyeeyesa omuntu nga abadde agudde mu mazzi \n",
    "aleme okumira naye ate okusinga byonna aleme okusaanawo.|<eos>| \n",
    "Nga ennaku bbiri zeklungudde okuva ku olwo, enkoko yakwatwa mumwa ne tutwalibwa \n",
    "kinoomu mu maaso g’ofiisa eyali akkalidde ku mmeeza okwali entuutuli y’empapula. Nga \n",
    "akutunuulidde mu munye enkakaba, yakukemesanga olufubenje lwebibuuzo ebyajjanga \n",
    "bisindikagana era nga osuubirwa okubiddamu byonna nga bwe byakuyigirizibwa. \n",
    "Byatandikanga bwebiti: “Erinnyalyo ggwe ani? (Eryange nno eryekiyugoyugo lyali Pwi \n",
    "Zaalizaali) “Wazaalibwa wa era ddi? Kitaawo ne nyoko be bani?webalu oba baafa? “ Ku ebyo \n",
    "ngobuulizibwako kajojijoji webirala lutottoebikwata ku mpisa, obuwangwa, obulombolombo \n",
    "n’ennono z’ebika byabayugoyugo. Wano munnaffe Ludyeku akakusu we kaamulijjira enkalu \n",
    "ne kamutandaggira ennume yekigwo n’awambira era bwatyo teyeeyongera kulamaga naffe \n",
    "okuva wano.|<eos>| \n",
    "Enjuba nga egolooba, enviiri zaffe zaababirwa ne zisikibwa ziwanvuwe nga eza Bayugoyugo. \n",
    "Ekyo bwe kyaggwa ne twambala ebyambalo ebyekinnansi era ne balikkupu okwali amalobo \n",
    "n’amasanda bituziyize okuseerera. Bwe twamala okuwenjebwa balabe oba nga tetuliiko \n",
    "kayinza kutuloopa konna singa nga tugwa mu mikono gya ba kawenkene, twassibwa mu \n",
    "kaato akomuliro ne katandika okuwenyuka. Nga obudde bukunukkiriza akawozamasiga, \n",
    "akaato kaggyibwaamu omuliro ne katandika kuseeyeeyeza ku maanyi ga masannyalaze \n",
    "kasirise nnyo era kasoobo. Mu kiseera kyekimu twawulira okubwatuka kw'emizinga gyerimu \n",
    "ku maato gaffe nga gibunduggula amasasi ku mwalo ogwatuli ku ddyo mailo nga musanvu \n",
    "okuva we twaali, ate nga ku kkono waffe ennyonyi ziwandagaza ku lusiisira lwabajaasi \n",
    "b’omulabe.|<eos>| \n",
    "ESSUULA EYOKUBIRI: Nsimba Ekigere mu Yugoyugo\n",
    "Nga tumulisibwa okumyansa kw’emizinga n’okutulika kwa bbomu, twagenda nga \n",
    "twewagaanya mu lukono lw’ennyanja olubugiddwa agasenge g’enjazi ennangaavu ezeesimbye \n",
    "obulanga. Bwe twatuuka e kkomekkome w’olugomo olwo, munnaffe Taamutaamu yatagalala \n",
    "ku kaato akatengeetera, era nga akozesa obukugu obusukkirivu yakasuka omuguwa okwali \n",
    "empuluttulizo n’aloba ensoomi yomutwe ogwali ku lukolekole lw’ejjinja. Kino lya tulaga nti \n",
    "munnaffe yali lugo olumanyi embuzi emponge mu bitundu bino.   Ku muguwa guno kwe \n",
    "twawalampira kinoomu nga tuyambibwa balikkupu zaffe okutuusa ffenna lwe twaggwaayo. \n",
    "Bwe twatuuka waggulu buli omu nanaanulamu balikkupu ze ne tuzisuula mu kaato wansi \n",
    "ebifuba nga kkumi na bitaano okuva we twaali. Twanaanika mu bigere engatto zi-nkya ne \n",
    "tutambula mu lwakasota nga tugenda twewagaanya mu mpago zenjazi awamu nga tutandira \n",
    "nokulandira ku zo ate nga awalala nga tuyuuguumira tunyegerera ku mkiribi \n",
    "n’eŋŋongogongo zaago.|<eos>| \n",
    "Mu kawozamasiga twagguka ku liyumba eritimbaganye ebimli nga biriboyedde lyonna era \n",
    "nga nebimu bireebeetera mu kisasi. Mwali mwakamu etaala ensiikirize, kimpoowooze. \n",
    "Twaatambula kasoobo (twerinnyako) nga tugenda tulikiiba naye nga tuliyita kumpi ddala \n",
    "emabega wakakomera akaali kajjudde enkanaga ne kyukompoleze. Nga tweekiise eriyumba \n",
    "eriyumba eryo mu bwanga, twalengera abajaasi abaJapan babiri nga boogera biwanvuwanvu, \n",
    "omu ku bon go officer. Mu bye nasobola okunojjola mu lukunkumuli lwebigambo bye baal \n",
    "bafukumula, kyawulikika nga baali basowaganira naŋŋanda eyali afaabina nga bwasobola \n",
    "okubatawulula naye ng’afuuwa mukka mu kisero. |<eos>| \n",
    "Twayimirira mu kakuukuulu nga mpaawo anyega ne tugabeegeka. Omujaasi ataali Offisa \n",
    "yasowola ekitala nakigalula nga alinga agenda okusanjaga Ofiisa, naye nga omukonogwe \n",
    "gukyakongobadde waggulu, Ofiisa yasika basitoola ku lukugunyu n’amubabika essasi \n",
    "n’ekitala nekimansuka eri. |<eos>| \n",
    "Omukazi yabikka engalo ze ku mumwa n’abaaluuka nnyo nga n’amaaso mu kiwanga \n",
    "tasigazzaamu. Nga afunyeeko are nga awambaatidde oluveeralwekiteeteeyi kye yatyagira ne \n",
    "yevumba ennyumba. Omukube yadduka nga akutte omukonogwe mu kifuba, nga \n",
    "abendabenda era akotyonka, n’agezaako alinnyelinnye amadaala ayingire mu nnyumba. \n",
    "Ettaala zo ku lubalaza zaatandika okumemuka.  Nga amalako amadaala, yalemererwa nagwa \n",
    "nga yevuunise. Offiisa yasooka okumugoberezesa amaaso noluvannyuma ye yennyini \n",
    "yamulumba. Bwe yamutuukako namutunuulira ng’akimba era nga yenyinyimbwa. \n",
    "Yamusindisa ekigere era omukube eyalabika ng’afudde yayiringitira ku madaala okutuusa \n",
    "lwe yawagamira mu kisasi awatonnya amazzi. Ofiisa nga abiina ensige nga nemikono gimuli \n",
    "mu nsawo yakyuka ayingire era twawulira amaloboozi agavuuvuuma naga gava mu nju gajja \n",
    "gasembera era ne tulengera nabantu nga bewungulawungula. Twaleka biri bwebityonga \n",
    "tugumbulukuka kukunta ku zaffe. |<eos>| \n",
    "Mu kawansazi (ttumbi)twagguka  mu kikubo ekyanfaananira ekikuute ky’ente. Taamutaamu \n",
    "yasindogoma nga ekiwuugulu. Eddoboozi ne limwanukula mu ngeri y’emu nga liyima mu \n",
    "gayinja agaali ku mabbali g’ekikubo. Eddoboozi lyelimu lyakaaba ng’ettutuma. Taamutaamu \n",
    "n’alyanukula nga akaaba ng’olubugabuga. Nga wayiseewo akaseera nalengera ekintu \n",
    "ekizijjagavu nga kyekkata mu kkubo okuva mu mayinja eddoboozi gye lyafulumanga.   \n",
    "ŋŋenda okukyetegereza nga nga ndaba muntu asakaatidde mu byambalo.\n",
    "Taamutaamu yadduka okumusisinkana era nga bamaze okugwaŋŋana mu bifuba, oli \n",
    "yagendako mu maaso katono nakukunula mu kasaka ekigaali ekyali kiwalulwa ensolo bbiri. \n",
    "Taamutaamu yasembera natuwenya tugende tulinye mu kigaali ekyaali kifaananako \n",
    "“kasimby’obwaala”. Nga tumaze okukkalira mu kyo, kyaatandika okutukunguzza nga \n",
    "bwekigenda kiguluba. Nali ntandise okutema ebisiki ne nsisimulwa okweggunda n’okuyuuga \n",
    "eby’ekitalo.|<eos>| \n",
    "Nga nyiimudde amaaso, nalaba ogusolo emabega waffe nga gutumezeeko. Gwabuuka \n",
    "enfunda ssatu zokka nga gwerippye dda ku lubugirizo lw’ekigaali. Gwansikulako Kataayi ku \n",
    "lusegere era nze okugenda okugezaako okumubakirira nga gwakuunye dda naye. Ekigaali \n",
    "tekyayimirira wadde okukendeeza ku misinde gyakyo era engeri gye kyesulikangamu nga \n",
    "kiweta ensonda, nga ekutabangula n’ebyenda are nga ddala tolowooza nti kinaddayo \n",
    "okutereera.  Naye nga tumazeeko obubirabira, omugoba yassa ekikkowe era nekisala ku \n",
    "mbiro zaakyo. Nga bwanyeenya omutwe yagamba nti, “Kino kitalo. Oyo munnaffe alugenze, \n",
    "jjagwa emulidde. Owa, obulamu buno buzibu!” Ensolo eno jjagwa etiibwa nnyo mu bitundu \n",
    "bino. Mu nkula nenfaanana eri wakati wa genge n’engo naye zombie ezireebeeseza wala mu \n",
    "maanyi, mu bukalabakalaba ne mu bukambwe. Abantu abali wakati w’enkumi ennya \n",
    "nenkumi ettaano be babweebweenebwa ensolo eno buli mwakaate abalala nga enkumi bbiri \n",
    "ne batirimbulwa ekisota ki tipitipi. Ekisot kino kikunguyivu nnyo era kifaananamu kaamuje. \n",
    "Kirina erinnyo liringa ejjindu lya sseggwanga are kya busagwa buyitirivu. Bwe kibojja \n",
    "omuntu afiirawo mbulaga ate bakibusalako busazi. Ekisingako ggwe kwe kukyesalako nga \n",
    "kyakakubojja sso notopaapaala bupaapaazi. Lugaba bwaba ng’akukwatiddeko oyinza \n",
    "okusumattuka  amagombe.|<eos>| \n",
    "Obudde buba busaasaana ne tuva ku kigaali ne tukwata akakubo akamawunjuwunju. \n",
    "Oluvannyuma lw’akatemerero, twaakavaamu ne tubandira mu nsiko. Obudde nga \n",
    "butangadde, twagguka mu kiwonvu ekyaali kumpi kyetooloddwa ebisozisozi kyonna, nga \n",
    "kirimu n’ogukonko omwali omugga ogutokota n’okusaala mu ngeri eyekitalo. Gwaali \n",
    "gwesengeseko agayinja n’enjazi ku mbangabanga zaagwo. Nalagibwa oluwokowoko mu jjinja \n",
    "okwali linnaalyo nga liryetengereddeko ng’erigenda okunegukako, ne ndagirwa nekukume \n",
    "omwo era nkozese ne byonna ebyalimu.|<eos>| \n",
    "Nze wano natandika okutya nga ndowooza nti oba nga abaJapan bawulidde omusinde \n",
    "gw’emmundu era nga bajja baguwondera, ddala nali mu katyabaga. Ate nendowooza nti \n",
    "omuwala bwanaddayo tajja kulema kulaalaasa bibaddewo era n’abantu abalala okubisansa ne \n",
    "bituuka ne ku ba kafulu. |<eos>| \n",
    "Nga nkyali mu birowoozo ebinnyogovu ng’ebyo ntongeza, Taamutaamu nagoba. Ddala \n",
    "yansala ku gwa kabugu. Yalabika nga munyiikaavu mpozzi olw’okunsanga n’omuwala. Naye \n",
    "sikutendera ngeri gye yatunulamu ng’amaasoge gagudde ku mutulumbi gwa jjagwa. \n",
    "Ng’ayasaamiridde, ensaya atadde, amaaso gakoonose mu kiwanga, yatunuulira jjagwa \n",
    "ng’anaagimira. Bwagigasimbulako nagazza ku ffe ate n’agigazzaako ate n’agazza. Ng’ali mu \n",
    "mbeera bwetyo, omuwala yevaamu namutegeeza byonna ebibaddewo. Yampomeka eriiso nga \n",
    "bwafuuwa ekiwa ng’akizza munda, ng’anyeenya n’omutwe, yagamba nti, “Oli musajja \n",
    "musajja wattu.” Yazza omuwala kubbali n’amukuba akaama. Oluvannyuma yajja nantegeeza \n",
    "nga bweyali agenda okuwerekera ki Wi (eryo lye lyali erinnya ly’omuwala) amuzzeeyo \n",
    "ewaabwe.|<eos>| |<endoftext>|]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ef52c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MyTokenizer(text, vocab_size=400, allowed_special=[\"|<endoftext>|\", \"|<eos>|\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f43e9117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging the 0th pair\n",
      "Merging the 100th pair\n",
      "tokens length:  13908\n",
      "ids length:  6886\n",
      "compression ratio: 2.02X\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "tokenizer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c8bc494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: b'\\x00',\n",
       " 1: b'\\x01',\n",
       " 2: b'\\x02',\n",
       " 3: b'\\x03',\n",
       " 4: b'\\x04',\n",
       " 5: b'\\x05',\n",
       " 6: b'\\x06',\n",
       " 7: b'\\x07',\n",
       " 8: b'\\x08',\n",
       " 9: b'\\t',\n",
       " 10: b'\\n',\n",
       " 11: b'\\x0b',\n",
       " 12: b'\\x0c',\n",
       " 13: b'\\r',\n",
       " 14: b'\\x0e',\n",
       " 15: b'\\x0f',\n",
       " 16: b'\\x10',\n",
       " 17: b'\\x11',\n",
       " 18: b'\\x12',\n",
       " 19: b'\\x13',\n",
       " 20: b'\\x14',\n",
       " 21: b'\\x15',\n",
       " 22: b'\\x16',\n",
       " 23: b'\\x17',\n",
       " 24: b'\\x18',\n",
       " 25: b'\\x19',\n",
       " 26: b'\\x1a',\n",
       " 27: b'\\x1b',\n",
       " 28: b'\\x1c',\n",
       " 29: b'\\x1d',\n",
       " 30: b'\\x1e',\n",
       " 31: b'\\x1f',\n",
       " 32: b' ',\n",
       " 33: b'!',\n",
       " 34: b'\"',\n",
       " 35: b'#',\n",
       " 36: b'$',\n",
       " 37: b'%',\n",
       " 38: b'&',\n",
       " 39: b\"'\",\n",
       " 40: b'(',\n",
       " 41: b')',\n",
       " 42: b'*',\n",
       " 43: b'+',\n",
       " 44: b',',\n",
       " 45: b'-',\n",
       " 46: b'.',\n",
       " 47: b'/',\n",
       " 48: b'0',\n",
       " 49: b'1',\n",
       " 50: b'2',\n",
       " 51: b'3',\n",
       " 52: b'4',\n",
       " 53: b'5',\n",
       " 54: b'6',\n",
       " 55: b'7',\n",
       " 56: b'8',\n",
       " 57: b'9',\n",
       " 58: b':',\n",
       " 59: b';',\n",
       " 60: b'<',\n",
       " 61: b'=',\n",
       " 62: b'>',\n",
       " 63: b'?',\n",
       " 64: b'@',\n",
       " 65: b'A',\n",
       " 66: b'B',\n",
       " 67: b'C',\n",
       " 68: b'D',\n",
       " 69: b'E',\n",
       " 70: b'F',\n",
       " 71: b'G',\n",
       " 72: b'H',\n",
       " 73: b'I',\n",
       " 74: b'J',\n",
       " 75: b'K',\n",
       " 76: b'L',\n",
       " 77: b'M',\n",
       " 78: b'N',\n",
       " 79: b'O',\n",
       " 80: b'P',\n",
       " 81: b'Q',\n",
       " 82: b'R',\n",
       " 83: b'S',\n",
       " 84: b'T',\n",
       " 85: b'U',\n",
       " 86: b'V',\n",
       " 87: b'W',\n",
       " 88: b'X',\n",
       " 89: b'Y',\n",
       " 90: b'Z',\n",
       " 91: b'[',\n",
       " 92: b'\\\\',\n",
       " 93: b']',\n",
       " 94: b'^',\n",
       " 95: b'_',\n",
       " 96: b'`',\n",
       " 97: b'a',\n",
       " 98: b'b',\n",
       " 99: b'c',\n",
       " 100: b'd',\n",
       " 101: b'e',\n",
       " 102: b'f',\n",
       " 103: b'g',\n",
       " 104: b'h',\n",
       " 105: b'i',\n",
       " 106: b'j',\n",
       " 107: b'k',\n",
       " 108: b'l',\n",
       " 109: b'm',\n",
       " 110: b'n',\n",
       " 111: b'o',\n",
       " 112: b'p',\n",
       " 113: b'q',\n",
       " 114: b'r',\n",
       " 115: b's',\n",
       " 116: b't',\n",
       " 117: b'u',\n",
       " 118: b'v',\n",
       " 119: b'w',\n",
       " 120: b'x',\n",
       " 121: b'y',\n",
       " 122: b'z',\n",
       " 123: b'{',\n",
       " 124: b'|',\n",
       " 125: b'}',\n",
       " 126: b'~',\n",
       " 127: b'\\x7f',\n",
       " 128: b'\\x80',\n",
       " 129: b'\\x81',\n",
       " 130: b'\\x82',\n",
       " 131: b'\\x83',\n",
       " 132: b'\\x84',\n",
       " 133: b'\\x85',\n",
       " 134: b'\\x86',\n",
       " 135: b'\\x87',\n",
       " 136: b'\\x88',\n",
       " 137: b'\\x89',\n",
       " 138: b'\\x8a',\n",
       " 139: b'\\x8b',\n",
       " 140: b'\\x8c',\n",
       " 141: b'\\x8d',\n",
       " 142: b'\\x8e',\n",
       " 143: b'\\x8f',\n",
       " 144: b'\\x90',\n",
       " 145: b'\\x91',\n",
       " 146: b'\\x92',\n",
       " 147: b'\\x93',\n",
       " 148: b'\\x94',\n",
       " 149: b'\\x95',\n",
       " 150: b'\\x96',\n",
       " 151: b'\\x97',\n",
       " 152: b'\\x98',\n",
       " 153: b'\\x99',\n",
       " 154: b'\\x9a',\n",
       " 155: b'\\x9b',\n",
       " 156: b'\\x9c',\n",
       " 157: b'\\x9d',\n",
       " 158: b'\\x9e',\n",
       " 159: b'\\x9f',\n",
       " 160: b'\\xa0',\n",
       " 161: b'\\xa1',\n",
       " 162: b'\\xa2',\n",
       " 163: b'\\xa3',\n",
       " 164: b'\\xa4',\n",
       " 165: b'\\xa5',\n",
       " 166: b'\\xa6',\n",
       " 167: b'\\xa7',\n",
       " 168: b'\\xa8',\n",
       " 169: b'\\xa9',\n",
       " 170: b'\\xaa',\n",
       " 171: b'\\xab',\n",
       " 172: b'\\xac',\n",
       " 173: b'\\xad',\n",
       " 174: b'\\xae',\n",
       " 175: b'\\xaf',\n",
       " 176: b'\\xb0',\n",
       " 177: b'\\xb1',\n",
       " 178: b'\\xb2',\n",
       " 179: b'\\xb3',\n",
       " 180: b'\\xb4',\n",
       " 181: b'\\xb5',\n",
       " 182: b'\\xb6',\n",
       " 183: b'\\xb7',\n",
       " 184: b'\\xb8',\n",
       " 185: b'\\xb9',\n",
       " 186: b'\\xba',\n",
       " 187: b'\\xbb',\n",
       " 188: b'\\xbc',\n",
       " 189: b'\\xbd',\n",
       " 190: b'\\xbe',\n",
       " 191: b'\\xbf',\n",
       " 192: b'\\xc0',\n",
       " 193: b'\\xc1',\n",
       " 194: b'\\xc2',\n",
       " 195: b'\\xc3',\n",
       " 196: b'\\xc4',\n",
       " 197: b'\\xc5',\n",
       " 198: b'\\xc6',\n",
       " 199: b'\\xc7',\n",
       " 200: b'\\xc8',\n",
       " 201: b'\\xc9',\n",
       " 202: b'\\xca',\n",
       " 203: b'\\xcb',\n",
       " 204: b'\\xcc',\n",
       " 205: b'\\xcd',\n",
       " 206: b'\\xce',\n",
       " 207: b'\\xcf',\n",
       " 208: b'\\xd0',\n",
       " 209: b'\\xd1',\n",
       " 210: b'\\xd2',\n",
       " 211: b'\\xd3',\n",
       " 212: b'\\xd4',\n",
       " 213: b'\\xd5',\n",
       " 214: b'\\xd6',\n",
       " 215: b'\\xd7',\n",
       " 216: b'\\xd8',\n",
       " 217: b'\\xd9',\n",
       " 218: b'\\xda',\n",
       " 219: b'\\xdb',\n",
       " 220: b'\\xdc',\n",
       " 221: b'\\xdd',\n",
       " 222: b'\\xde',\n",
       " 223: b'\\xdf',\n",
       " 224: b'\\xe0',\n",
       " 225: b'\\xe1',\n",
       " 226: b'\\xe2',\n",
       " 227: b'\\xe3',\n",
       " 228: b'\\xe4',\n",
       " 229: b'\\xe5',\n",
       " 230: b'\\xe6',\n",
       " 231: b'\\xe7',\n",
       " 232: b'\\xe8',\n",
       " 233: b'\\xe9',\n",
       " 234: b'\\xea',\n",
       " 235: b'\\xeb',\n",
       " 236: b'\\xec',\n",
       " 237: b'\\xed',\n",
       " 238: b'\\xee',\n",
       " 239: b'\\xef',\n",
       " 240: b'\\xf0',\n",
       " 241: b'\\xf1',\n",
       " 242: b'\\xf2',\n",
       " 243: b'\\xf3',\n",
       " 244: b'\\xf4',\n",
       " 245: b'\\xf5',\n",
       " 246: b'\\xf6',\n",
       " 247: b'\\xf7',\n",
       " 248: b'\\xf8',\n",
       " 249: b'\\xf9',\n",
       " 250: b'\\xfa',\n",
       " 251: b'\\xfb',\n",
       " 252: b'\\xfc',\n",
       " 253: b'\\xfd',\n",
       " 254: b'\\xfe',\n",
       " 255: b'\\xff',\n",
       " 256: b'a ',\n",
       " 257: b'e ',\n",
       " 258: b'u ',\n",
       " 259: b'ng',\n",
       " 260: b'o ',\n",
       " 261: b'al',\n",
       " 262: b'ku',\n",
       " 263: b'i ',\n",
       " 264: b'wa',\n",
       " 265: b'er',\n",
       " 266: b'nga ',\n",
       " 267: b'an',\n",
       " 268: b'ir',\n",
       " 269: b'mu ',\n",
       " 270: b'ul',\n",
       " 271: b'en',\n",
       " 272: b'. ',\n",
       " 273: b'aa',\n",
       " 274: b'\\xe2\\x80',\n",
       " 275: b'ki',\n",
       " 276: b'\\xe2\\x80\\x99',\n",
       " 277: b'tu',\n",
       " 278: b'mu',\n",
       " 279: b', ',\n",
       " 280: b'ba',\n",
       " 281: b'a n',\n",
       " 282: b'oku',\n",
       " 283: b'ka',\n",
       " 284: b'dd',\n",
       " 285: b'ya',\n",
       " 286: b'ny',\n",
       " 287: b'ga',\n",
       " 288: b'bi',\n",
       " 289: b'mb',\n",
       " 290: b'ka ',\n",
       " 291: b'wa ',\n",
       " 292: b'ku ',\n",
       " 293: b'ali ',\n",
       " 294: b'ol',\n",
       " 295: b'si',\n",
       " 296: b'na',\n",
       " 297: b'era ',\n",
       " 298: b'we ',\n",
       " 299: b'ko ',\n",
       " 300: b'bu',\n",
       " 301: b'ma',\n",
       " 302: b'zi',\n",
       " 303: b'we',\n",
       " 304: b'ye ',\n",
       " 305: b'ye',\n",
       " 306: b'ko',\n",
       " 307: b'nd',\n",
       " 308: b'ira ',\n",
       " 309: b'ala ',\n",
       " 310: b'gu',\n",
       " 311: b'eb',\n",
       " 312: b'ab',\n",
       " 313: b'yi',\n",
       " 314: b'dde ',\n",
       " 315: b'za ',\n",
       " 316: b'ga ',\n",
       " 317: b'yu',\n",
       " 318: b'ob',\n",
       " 319: b'jj',\n",
       " 320: b'eki',\n",
       " 321: b'twa',\n",
       " 322: b'em',\n",
       " 323: b'iri',\n",
       " 324: b'end',\n",
       " 325: b'sa',\n",
       " 326: b'yo ',\n",
       " 327: b'nny',\n",
       " 328: b'a ne ',\n",
       " 329: b'ag',\n",
       " 330: b'ee',\n",
       " 331: b'ff',\n",
       " 332: b'yo',\n",
       " 333: b'oo',\n",
       " 334: b'no ',\n",
       " 335: b'ya ',\n",
       " 336: b'go',\n",
       " 337: b'n\\xe2\\x80\\x99',\n",
       " 338: b'ek',\n",
       " 339: b'ala',\n",
       " 340: b'ali',\n",
       " 341: b'lu',\n",
       " 342: b'sa ',\n",
       " 343: b'ne ',\n",
       " 344: b'ta',\n",
       " 345: b' \\n',\n",
       " 346: b'omu',\n",
       " 347: b'na ',\n",
       " 348: b'zi ',\n",
       " 349: b'tul',\n",
       " 350: b'eng',\n",
       " 351: b'gg',\n",
       " 352: b'mul',\n",
       " 353: b'aat',\n",
       " 354: b'|<eos>| \\n',\n",
       " 355: b'and',\n",
       " 356: b'to',\n",
       " 357: b'ffe ',\n",
       " 358: b'te ',\n",
       " 359: b'so',\n",
       " 360: b'kub',\n",
       " 361: b'. N',\n",
       " 362: b'um',\n",
       " 363: b'eri',\n",
       " 364: b'nga \\n',\n",
       " 365: b'. \\n',\n",
       " 366: b'wo',\n",
       " 367: b'wo ',\n",
       " 368: b'se',\n",
       " 369: b'fu',\n",
       " 370: b'enda ',\n",
       " 371: b'za',\n",
       " 372: b'am',\n",
       " 373: b'si ',\n",
       " 374: b'nj',\n",
       " 375: b'ze ',\n",
       " 376: b'bwa ',\n",
       " 377: b'w\\xe2\\x80\\x99',\n",
       " 378: b'ebi',\n",
       " 379: b'era n',\n",
       " 380: b'a mu ',\n",
       " 381: b'aka',\n",
       " 382: b'zaa',\n",
       " 383: b'.|<eos>| \\n',\n",
       " 384: b'mp',\n",
       " 385: b'li',\n",
       " 386: b'\\xc5\\x8b',\n",
       " 387: b'te',\n",
       " 388: b'aga',\n",
       " 389: b'. O',\n",
       " 390: b'nt',\n",
       " 391: b'tuu',\n",
       " 392: b'ama',\n",
       " 393: b'ula',\n",
       " 394: b'ulu',\n",
       " 395: b'a \\n',\n",
       " 396: b'ti',\n",
       " 397: b'su',\n",
       " 398: b'bir',\n",
       " 399: b'bwa',\n",
       " 400: b'|<endoftext>|',\n",
       " 401: b'|<eos>|'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44503546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of original tokens:  14\n",
      "Length of tokens after handling special words:  14\n",
      "Length of tokens after merging:  12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[72, 101, 108, 108, 260, 366, 114, 108, 100, 33, 33, 33]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_s = tokenizer.encode(\"Hello world!!!\")\n",
    "id_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46918725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!!!\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.decode(id_s)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1103b215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of original tokens:  41\n",
      "Length of tokens after handling special words:  23\n",
      "Length of tokens after merging:  17\n",
      "[72, 101, 121, 32, 116, 104, 265, 257, 401, 32, 71, 305, 98, 261, 101, 299, 400]\n"
     ]
    }
   ],
   "source": [
    "#with special tokens\n",
    "id_s2 = tokenizer.encode(\"Hey there |<eos>| Gyebaleko |<endoftext>|\")\n",
    "print(id_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b003a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey there |<eos>| Gyebaleko |<endoftext>|\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.decode(id_s2)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330d4c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac81fa33",
   "metadata": {},
   "source": [
    "# usage\n",
    "1. prepare corpus/text data with relevant special tokens if need be\n",
    "2. call the tokenizer class with input parameters, text, vocab_size and allowed_special characters \n",
    "    e.g `tokenizer = MyTokenizer(text, vocab_size=500, allowed_special=[\"|<endoftext>|\", \"|<eos>|\"])`\n",
    "3. train the tokenizer\n",
    "    e.g `tokenizer.train()`\n",
    "4. then use it to encode any new text to get tokens\n",
    "    e.g `ids = tokenizer.encode(\"Hello world\")`\n",
    "5. you can finally do any decoding\n",
    "    e.g `tokenizer.decode(ids)`\n",
    "6. to get vocabulary, u can use the get_vocabulary function\n",
    "   e.g `tokenizer.get_vocabulary()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80054a01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
